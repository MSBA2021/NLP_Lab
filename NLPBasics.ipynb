{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP): Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By: Dr. Reza Mousavi, University of Virginia, mousavi@virginia.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we cover basic concepts in Natural Language Processing (NLP). We start with an introduction to NLP, then we cover pre-processing text, and finally, we will covert dictionary-based approaches in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the emergence and proliferation of automated text mining approaches that we deploy these days, manual text mining approaches surfaced in mid 1980’s. The challenge of exploiting the large proportion of enterprise information that originates in “unstructured” form was first recognized in an IBM article by H.P. Luhn. As BI emerged in the 80s and 90s as a software category, the emphasis was on numerical data stored in relational databases.\n",
    "\n",
    "By the booming of the World Wide Web and then the proliferation of social media websites, humans created a universal repository of knowledge such that almost any kind of knowledge can be found there. The users pay little to access the web. There is no central editorial board in the web, so anyone can contribute to the web. These properties stimulate the users to retrieve information from the web and add information to the web, making the web grow in an incredible speed.\n",
    "\n",
    "Along with the availability of text due to the growth in the web and social media, technological advances (hardware and software) have enabled the field to advance during the past decade. Furthermore, the evolution of artificial intelligence urged scholars and practitioners to explore beyond the structured data and apply their models to unstructured data and particularly text data. Therefore the problem of text mining, **i.e. discovering useful knowledge from unstructured or semi-structured text,** has been becoming more and more attractive.\n",
    "\n",
    "There are many applications for text mining techniques. A few examples include:\n",
    "\n",
    "* Security applications: Monitoring the text generated online to identify threats.\n",
    "\n",
    "* Biomedical applications: knowledge based search engine for biomedical texts.\n",
    "\n",
    "* Marketing applications: analytical customer relationship management, advertisement, and product development. Please read “Sentiment Analysis Can Do More than Prevent Fraud and Turnover” by Michael Schrage (available in module Week 6).\n",
    "\n",
    "* Academia: To study human behavior in a social environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Is Text Analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP, Text Analytics, and Text Mining (these terms are used interchangeably) is about looking for patterns in text. However, the superficial similarity between the two conceals real differences. Data mining can be more fully characterized as the extraction of implicit, previously unknown, and potentially useful information from data. The information is implicit in the input data: it is hidden, unknown, and could hardly be extracted without recourse to automatic techniques of data mining. With text mining, however, the information to be extracted is clearly and explicitly stated in the text. It’s not hidden at all—most authors go to great pains to make sure that they express themselves clearly and unambiguously—and, from a human point of view, the only sense in which it is “previously unknown” is that human resource restrictions make it infeasible for people to read the text themselves. The problem, of course, is that the information is not couched in a manner that is amenable to automatic processing. Text mining strives to bring it out of the text in a form that is suitable for consumption by computers directly, with no need for a human intermediary.\n",
    "\n",
    "Though there is a clear difference philosophically, from the computer’s point of view the problems are quite similar. Text is just as opaque as raw data when it comes to extracting information— probably more so.\n",
    "\n",
    "Another requirement that is common to both data and text mining is that the information extracted should be “potentially useful.” In one sense, this means actionable—capable of providing a basis for actions to be taken automatically. In the case of data mining, this notion can be expressed in a relatively domain-independent way: actionable patterns are ones that allow non-trivial predictions to be made on new data from the same source. Performance can be measured by counting successes and failures, statistical techniques can be applied to compare different data mining methods on the same problem, and so on. However, in many text mining situations it is far harder to characterize what “actionable” means in a way that is independent of the particular domain at hand. This makes it difficult to find fair and objective measures of success.\n",
    "\n",
    "In many data mining applications, “potentially useful” is given a different interpretation: the key for success is that the information extracted must be comprehensible in that it helps to explain the data. This is necessary whenever the result is intended for human consumption rather than (or as well as) a basis for automatic action. This criterion is less applicable to text mining because, unlike data mining, the input itself is comprehensible. Text mining with comprehensible output is tantamount to summarizing salient features from a large body of text, which is a subfield in its own right: text summarization.\n",
    "\n",
    "The Text Mining (TM) methods usually involve the following steps:\n",
    "\n",
    "* Basic pre-processing operations such as tokenization, stemming, and removing stop words.\n",
    "\n",
    "* Advanced text mining operations, involving identification of complex patterns. For instance, we can identify the main topics that were discussed in the text or we can quantify the sentiment in the text.\n",
    "\n",
    "TM exploits techniques and methodologies from data mining, machine learning, information retrieval, corpus-based computational linguistics. The main objective in TM is to extract useful information from the text documents. There are many different methods used in TM. In this class, we focus on the following areas: Pre-processing the Text, Sentiment Analysis, and Topic Modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of pre-processing the text documents is to prepare the text for TM methods. Depending on the type of TM method that we want to deploy, there are different pre-processing steps that we should take to prepare our text data. The main steps include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens. The list of tokens becomes input for further processing. Electronic text is a linear sequence of symbols (characters or words or phrases). Naturally, before any real text processing is to be done, text needs to be segmented into linguistic units such as words, punctuation, numbers, alpha-numerics, etc. This process is called tokenization.\n",
    "\n",
    "In English, words are often separated from each other by blanks (white space), but not all white space is equal. Both “Los Angeles” and “rock ‘n’ roll” are individual thoughts despite the fact that they contain multiple words and spaces. We may also need to separate single words like “I’m” into separate words “I” and “am”.\n",
    "\n",
    "Tokenization is a kind of pre-processing in a sense; an identification of basic units to be processed. It is conventional to concentrate on pure analysis or generation while taking basic units for granted. Yet without these basic units clearly segregated it is impossible to carry out any analysis or generation.\n",
    "\n",
    "The most common method for tokenization is called “standard (white space) tokenization”. In the standard tokenization, each word that has a white space before or after its location in the text will be considered a token. For instance, there are four tokens in the following text: “This is an example”. Those are “This”, “is”, “an”, and “example”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. True-casing (convert to lower/ upper case):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truecasing is the problem in natural language processing (NLP) of determining the proper capitalization of words where such information is unavailable. This commonly comes up due to the standard practice (in English and many other languages) of automatically capitalizing the first word of a sentence.\n",
    "\n",
    "After tokenization, we usually need to convert all of the tokens into lower (or upper) case. This way, the software would not assume any difference between “Good” and “good”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Stop-word removal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important pre-processing step is to remove stop-words from the text. Each language has its own list of stop-words. A comprehensive list of stop-words for many languages can be found here: http://www.ranks.nl/stopwords. Examples of stop-words include “a”, “an”, and “did”. Stop-words are basically a set of commonly used words in any language, not just English. The reason why stop-words are critical to many applications is that, if we remove the words that are very commonly used in a given language, we can focus on the important words instead. For example, in the context of a search engine, if your search query is “how to develop information retrieval applications”, if the search engine tries to find web pages that contained the terms “how”, “to”, “develop”, “information”, “retrieval”, “applications” the search engine is going to find a lot more pages that contain the terms “how” and “to” than pages that contain information about developing information retrieval applications because the terms “how” and “to” are so commonly used in the English language. So, if we disregard these two terms, the search engine can actually focus on retrieving pages that contain the keywords: “develop”, “information”, “retrieval”, “applications” – which would more closely bring up pages that are really of interest. This is just the basic intuition for using stop-words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stemming & lemmatization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For grammatical reasons, documents are going to use different forms of a word, such as “organize”, “organizes”, and “organizing.” Additionally, there are families of derivationally related words with similar meanings, such as “democracy”, “democratic”, and “democratization.” In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "* am, are, is will change to be\n",
    "\n",
    "* car, cars, car’s, cars’ will change to car\n",
    "\n",
    "The result of this mapping of text will be something like: “the boy’s cars are different colors” will be changed to “the boy car be differ color”.\n",
    "\n",
    "However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. It is worth noting that most of the software packages focus on stemming but not lemmatization. It is also worth noting that the decision about applying lemmatization to the text data should be based on the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there are four main pre-processing steps before mining text. Using a real data set about customer service on Twitter, we learn about pre-processing text using Python package spaCy. This package is one of the best packages for NLP tasks and offers a wide range of capabilities. More information about spaCy can be found here: https://spacy.io  \n",
    "\n",
    "**For this class, we will work on a data set that was collected from Twitter. This data set contains tweets posted by Twitter users about the telecommunications firm \"Sprint\". \"Sprint\" is one of the firms that has a dedicated team of customer service representatives trained to address customer queries on Twitter.** \n",
    "\n",
    "\n",
    "Given the proliferation of Web 2.0 technology, customers are able to communicate with firms through new channels of communication such as social media websites. Almost two-thirds of customers have used a company's social media site to receive service. Such digitally provisioned service is attractive to customers due to its fast response, and is being increasingly preferred to more traditional service channels such as phone or email. Firms too have an incentive to embrace service provision through social media; the average cost of a service-related response in Twitter is only **one dollar**, while the average cost of interacting with a customer through a traditional call center can be close to **six dollars**. Furthermore, firms that use Twitter as a social care channel are seeing a large 19\\% increase in customer satisfaction. Given that digital customer care has mutual benefits for firms and customers, it is not surprising to observe a 250\\% increase in customer service interactions over Twitter during the past few years\n",
    "\n",
    "Given the size of the actual data, we only work with the first 5K tweets that were posted about sprint. The file name is tweets_about_sprint.csv. We begin with installing/ importing Python packages and reading the CSV file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nrclex\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: textblob in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from nrclex) (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from textblob->nrclex) (3.5)\n",
      "Requirement already satisfied: tqdm in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob->nrclex) (4.50.2)\n",
      "Requirement already satisfied: click in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob->nrclex) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob->nrclex) (0.17.0)\n",
      "Requirement already satisfied: regex in /Users/reza/opt/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob->nrclex) (2020.10.15)\n",
      "Building wheels for collected packages: nrclex\n",
      "  Building wheel for nrclex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43310 sha256=4609d5690b5839844b51218e8ea24965032bb601f7fac3c15600cfeb4637fb4d\n",
      "  Stored in directory: /Users/reza/Library/Caches/pip/wheels/83/95/c0/42b43fb15eb48e4f5a67cba8915540cb2783591c59c037a9e5\n",
      "Successfully built nrclex\n",
      "Installing collected packages: nrclex\n",
      "Successfully installed nrclex-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy # install spaCy\n",
    "!pip install tqdm # install tqdm package to display the progress\n",
    "!pip install textblob # for dictionary-based sentment analysis\n",
    "!pip install nrclex # for dictionary-based emotions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# import and load spaCy:\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other packages:\n",
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from nrclex import NRCLex\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PortAGroom</td>\n",
       "      <td>Retweeted Sprint sprint The on the left has 1 ...</td>\n",
       "      <td>6/27/16 6:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>androidNewsIn</td>\n",
       "      <td>T Mobile AT amp T Sprint and Verizon offer fre...</td>\n",
       "      <td>6/29/16 9:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FaithLunden</td>\n",
       "      <td>RT SprintPalmer Can you say almost there Keep ...</td>\n",
       "      <td>7/2/16 18:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jasleicol</td>\n",
       "      <td>RT t marieeeeee sprint how about LifeOfDesiign...</td>\n",
       "      <td>6/20/16 20:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CraigpParrish</td>\n",
       "      <td>RT MW55 Our NASCARONFOX NASCAR XFINITY season ...</td>\n",
       "      <td>6/20/16 16:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                                              tweet  \\\n",
       "0     PortAGroom  Retweeted Sprint sprint The on the left has 1 ...   \n",
       "1  androidNewsIn  T Mobile AT amp T Sprint and Verizon offer fre...   \n",
       "2    FaithLunden  RT SprintPalmer Can you say almost there Keep ...   \n",
       "3      jasleicol  RT t marieeeeee sprint how about LifeOfDesiign...   \n",
       "4  CraigpParrish  RT MW55 Our NASCARONFOX NASCAR XFINITY season ...   \n",
       "\n",
       "            date  \n",
       "0   6/27/16 6:54  \n",
       "1   6/29/16 9:21  \n",
       "2   7/2/16 18:51  \n",
       "3  6/20/16 20:42  \n",
       "4  6/20/16 16:00  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tweets_about_sprint.csv').sample(n = 5000, random_state=2).reset_index(drop = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spaCy's powerful tokenizer to parse our text. Let's take a look at some features of spaCy using an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "and\n",
      "Samsung\n",
      "are\n",
      "looking\n",
      "at\n",
      "#\n",
      "buy\n",
      "buying\n",
      "Pixar\n",
      "for\n",
      "100\n",
      "billion\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp_doc = nlp(\"Apple and Samsung are looking at #buy buying Pixar for 100 billion.\")\n",
    "\n",
    "for token in nlp_doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token in our text, we can get the lemmatized version of the token, check if it is alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple True False\n",
      "and and True True\n",
      "Samsung Samsung True False\n",
      "are be True True\n",
      "looking look True False\n",
      "at at True True\n",
      "# # False False\n",
      "buy buy True False\n",
      "buying buy True False\n",
      "Pixar Pixar True False\n",
      "for for True True\n",
      "100 100 False False\n",
      "billion billion True False\n",
      ". . False False\n"
     ]
    }
   ],
   "source": [
    "for token in nlp_doc:\n",
    "    print(token.text, # Returns the original form of the token\n",
    "          token.lemma_, # Returns the lemmatized version of the token\n",
    "          token.is_alpha, # Checks if the token consists of alphabetic characters.\n",
    "          token.is_stop # Checks if the token is a stop-word\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"747f3343608748e8bd95b4f5fdbc7086-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Samsung</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at #</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">buy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Pixar</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">100</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">billion.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 750.0,2.0 750.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210.0,354.0 L218.0,342.0 202.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-6\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-8\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,354.0 L1623.0,342.0 1607.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-747f3343608748e8bd95b4f5fdbc7086-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-747f3343608748e8bd95b4f5fdbc7086-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(nlp_doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** After you run the cell above, please select the cell again and click on interrupt the kernel icon from the notbook's menu bar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "Samsung 10 17 ORG\n",
      "Pixar 45 50 ORG\n",
      "100 billion 55 66 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Samsung\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " are looking at #buy buying \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pixar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(nlp_doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** After you run the cell above, please select the cell again and click on interrupt the kernel icon from the notbook's menu bar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package spaCy has the functionality to remove stop-words from our text. We can even customize the list of stop-words we want to remove from the text. For instance, let's add a couple of words to the list of stop-words to be removed from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new stop words: \n",
    "customize_stop_words = [\n",
    "    'attach','startup'\n",
    "]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the pre-processing steps to our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reza/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 5000/5000 [00:22<00:00, 221.00it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas() # To display the progress\n",
    "df['pr_tweet'] = df.tweet.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>pr_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PortAGroom</td>\n",
       "      <td>Retweeted Sprint sprint The on the left has 1 ...</td>\n",
       "      <td>6/27/16 6:54</td>\n",
       "      <td>retweete Sprint sprint left foam mean s worth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>androidNewsIn</td>\n",
       "      <td>T Mobile AT amp T Sprint and Verizon offer fre...</td>\n",
       "      <td>6/29/16 9:21</td>\n",
       "      <td>T Mobile amp T Sprint Verizon offer free call ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                                              tweet  \\\n",
       "0     PortAGroom  Retweeted Sprint sprint The on the left has 1 ...   \n",
       "1  androidNewsIn  T Mobile AT amp T Sprint and Verizon offer fre...   \n",
       "\n",
       "           date                                           pr_tweet  \n",
       "0  6/27/16 6:54  retweete Sprint sprint left foam mean s worth ...  \n",
       "1  6/29/16 9:21  T Mobile amp T Sprint Verizon offer free call ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Dictionary-based NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text data is pre-processed, we can analyze them, or we can use them to build machine learning models. There are certain things that we can do with text without even training a model. For instance, we can find the list of positive and negative words in our text data. Imagine that there is a list of positive words such as “happy”, “glad”, “nice”, etc. we can search our text data to find aby of these words. If we find many positive words in our text data, then perhaps our text carries a positive sentiment. On the other hand, if we find lots of negative words in our text document, then that document carries a negative sentiment. The absence of either positive words and negative words in our text could mean that we have a neutral text document. Therefore, to be able to determine the sentiment of a document, we need to have a list of words commonly associated with positive emotions, and a list of words commonly with negative words; something like a dictionary that has words as the keys of the dictionary and positive/ negative as the values for the keys. For example:\n",
    "\n",
    "**Sentiment_Dictionary = {‘happy’ : ’positive’, ‘glad’ : ’positive’, ‘sad’ : ‘negative’, ‘angry’ : ‘negative’}** \n",
    "\n",
    "This dictionary could contain a comprehensive list of words associated with positive/ negative words. We can then use such a dictionary to determine the sentiment in each text document. This is an unsupervised method as we don’t need to train any model to determine the sentiment of each text document. We just simply find positive/ negative words in our text and use a formula to calculate a sentiment score. The presence of adjectives in a text could also indicate that they are subjective (as opposed to objective) text. We can therefore calculate the subjectivity of the text documents as well.  \n",
    "\n",
    "Python package “TextBlob” offers such functionality. Let’s use it to calculate the sentiment score for each tweet in our example data:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 6895.49it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7004.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 22.6 ms, total: 1.44 s\n",
      "Wall time: 1.44 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from textblob import TextBlob\n",
    "\n",
    "df['subj'] = df.pr_tweet.progress_apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "df['sent'] = df.pr_tweet.progress_apply(lambda x: TextBlob(str(x)).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>pr_tweet</th>\n",
       "      <th>subj</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PortAGroom</td>\n",
       "      <td>Retweeted Sprint sprint The on the left has 1 ...</td>\n",
       "      <td>6/27/16 6:54</td>\n",
       "      <td>retweete Sprint sprint left foam mean s worth ...</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>-0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>androidNewsIn</td>\n",
       "      <td>T Mobile AT amp T Sprint and Verizon offer fre...</td>\n",
       "      <td>6/29/16 9:21</td>\n",
       "      <td>T Mobile amp T Sprint Verizon offer free call ...</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FaithLunden</td>\n",
       "      <td>RT SprintPalmer Can you say almost there Keep ...</td>\n",
       "      <td>7/2/16 18:51</td>\n",
       "      <td>RT SprintPalmer share free speaker follower sp...</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jasleicol</td>\n",
       "      <td>RT t marieeeeee sprint how about LifeOfDesiign...</td>\n",
       "      <td>6/20/16 20:42</td>\n",
       "      <td>RT t marieeeeee sprint LifeOfDesiigner get cal...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CraigpParrish</td>\n",
       "      <td>RT MW55 Our NASCARONFOX NASCAR XFINITY season ...</td>\n",
       "      <td>6/20/16 16:00</td>\n",
       "      <td>RT NASCARONFOX NASCAR XFINITY season fun year ...</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                                              tweet  \\\n",
       "0     PortAGroom  Retweeted Sprint sprint The on the left has 1 ...   \n",
       "1  androidNewsIn  T Mobile AT amp T Sprint and Verizon offer fre...   \n",
       "2    FaithLunden  RT SprintPalmer Can you say almost there Keep ...   \n",
       "3      jasleicol  RT t marieeeeee sprint how about LifeOfDesiign...   \n",
       "4  CraigpParrish  RT MW55 Our NASCARONFOX NASCAR XFINITY season ...   \n",
       "\n",
       "            date                                           pr_tweet    subj  \\\n",
       "0   6/27/16 6:54  retweete Sprint sprint left foam mean s worth ...  0.2625   \n",
       "1   6/29/16 9:21  T Mobile amp T Sprint Verizon offer free call ...  0.8000   \n",
       "2   7/2/16 18:51  RT SprintPalmer share free speaker follower sp...  0.8000   \n",
       "3  6/20/16 20:42  RT t marieeeeee sprint LifeOfDesiigner get cal...  0.0000   \n",
       "4  6/20/16 16:00  RT NASCARONFOX NASCAR XFINITY season fun year ...  0.2000   \n",
       "\n",
       "       sent  \n",
       "0 -0.004167  \n",
       "1  0.400000  \n",
       "2  0.400000  \n",
       "3  0.000000  \n",
       "4  0.300000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in learning to what extent our text documents contain specific emotions such as “fear”, “trust, “anger”, etc., we can use another Python package called “NRCLex”: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 4691.55it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4820.18it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4859.64it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4883.45it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4917.68it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4793.13it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4941.49it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4938.42it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4946.94it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 4912.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 115 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nrclex import NRCLex\n",
    "\n",
    "df['fear'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['fear'])\n",
    "df['anger'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['anger'])\n",
    "df['anticip'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['anticip'])\n",
    "df['trust'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['trust'])\n",
    "df['surprise'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['surprise'])\n",
    "df['positive'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['positive'])\n",
    "df['negative'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['negative'])\n",
    "df['sadness'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['sadness'])\n",
    "df['disgust'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['disgust'])\n",
    "df['joy'] = df.pr_tweet.progress_apply(lambda x: NRCLex(str(x)).affect_frequencies['joy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>pr_tweet</th>\n",
       "      <th>subj</th>\n",
       "      <th>sent</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PortAGroom</td>\n",
       "      <td>Retweeted Sprint sprint The on the left has 1 ...</td>\n",
       "      <td>6/27/16 6:54</td>\n",
       "      <td>retweete Sprint sprint left foam mean s worth ...</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>androidNewsIn</td>\n",
       "      <td>T Mobile AT amp T Sprint and Verizon offer fre...</td>\n",
       "      <td>6/29/16 9:21</td>\n",
       "      <td>T Mobile amp T Sprint Verizon offer free call ...</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FaithLunden</td>\n",
       "      <td>RT SprintPalmer Can you say almost there Keep ...</td>\n",
       "      <td>7/2/16 18:51</td>\n",
       "      <td>RT SprintPalmer share free speaker follower sp...</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jasleicol</td>\n",
       "      <td>RT t marieeeeee sprint how about LifeOfDesiign...</td>\n",
       "      <td>6/20/16 20:42</td>\n",
       "      <td>RT t marieeeeee sprint LifeOfDesiigner get cal...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CraigpParrish</td>\n",
       "      <td>RT MW55 Our NASCARONFOX NASCAR XFINITY season ...</td>\n",
       "      <td>6/20/16 16:00</td>\n",
       "      <td>RT NASCARONFOX NASCAR XFINITY season fun year ...</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                                              tweet  \\\n",
       "0     PortAGroom  Retweeted Sprint sprint The on the left has 1 ...   \n",
       "1  androidNewsIn  T Mobile AT amp T Sprint and Verizon offer fre...   \n",
       "2    FaithLunden  RT SprintPalmer Can you say almost there Keep ...   \n",
       "3      jasleicol  RT t marieeeeee sprint how about LifeOfDesiign...   \n",
       "4  CraigpParrish  RT MW55 Our NASCARONFOX NASCAR XFINITY season ...   \n",
       "\n",
       "            date                                           pr_tweet    subj  \\\n",
       "0   6/27/16 6:54  retweete Sprint sprint left foam mean s worth ...  0.2625   \n",
       "1   6/29/16 9:21  T Mobile amp T Sprint Verizon offer free call ...  0.8000   \n",
       "2   7/2/16 18:51  RT SprintPalmer share free speaker follower sp...  0.8000   \n",
       "3  6/20/16 20:42  RT t marieeeeee sprint LifeOfDesiigner get cal...  0.0000   \n",
       "4  6/20/16 16:00  RT NASCARONFOX NASCAR XFINITY season fun year ...  0.2000   \n",
       "\n",
       "       sent  fear  anger  anticip  trust  surprise  positive  negative  \\\n",
       "0 -0.004167   0.0    0.0      0.0  0.000     0.000      1.00       0.0   \n",
       "1  0.400000   0.2    0.2      0.0  0.000     0.000      0.20       0.2   \n",
       "2  0.400000   0.0    0.0      0.0  0.400     0.000      0.20       0.0   \n",
       "3  0.000000   0.0    0.0      0.0  0.000     0.000      0.00       0.0   \n",
       "4  0.300000   0.0    0.0      0.0  0.125     0.125      0.25       0.0   \n",
       "\n",
       "   sadness  disgust   joy  \n",
       "0      0.0      0.0  0.00  \n",
       "1      0.0      0.0  0.00  \n",
       "2      0.0      0.0  0.20  \n",
       "3      0.0      0.0  0.00  \n",
       "4      0.0      0.0  0.25  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
